{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15398782942892209426\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7692124160\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 7425195516099361906\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ColorConverter\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import csv\n",
    "import sklearn\n",
    "import random # for shuffle\n",
    "import time\n",
    "\n",
    "gc.collect()\n",
    "#import h5py\n",
    "\n",
    "# keras_tqdm = keras progress bar (some bugs without)\n",
    "# https://pypi.python.org/pypi/keras-tqdm\n",
    "# http://forums.fast.ai/t/jupyter-notebook-dies-freezes-during-training/2651/4\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Cropping2D, Lambda\n",
    "from keras.layers import Conv2D, Dropout, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "# NOTES FOR WRITEUP\n",
    "# data - intially provided data\n",
    "# a2 - extra data\n",
    "# a3 - extra data from harder track\n",
    "# a4 - extra data reverse\n",
    "# a5 - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10404\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "first = True\n",
    "with open('../DATA0/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        if not first:\n",
    "            samples.append(line)\n",
    "        else:\n",
    "            first = False\n",
    "            \n",
    "random.shuffle(samples)\n",
    "\n",
    "print(len(samples))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(len(samples))\n",
    "#print(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X, y = read_training_data_into_memory(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# driving_log.csv - header\n",
    "# center,left,right,steering,throttle,brake,speed\n",
    "\n",
    "# dealing with unbalanced data with class_weight\n",
    "# https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n",
    "\n",
    "#from sklearn.utils import class_weight\n",
    "import sklearn.utils.class_weight\n",
    "\n",
    "def generator(samples,batch_size=32, image_prefix_path=\"../DATA0/IMG/\"):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        random.shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            images = []\n",
    "            angles = []\n",
    "            angle_offsets = [0.0, 0.20, -0.20]\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                center_angle = float(batch_sample[3])\n",
    "\n",
    "                for image_position in range(3):\n",
    "                    image_subpath = batch_sample[image_position].split('/')[-1]\n",
    "                    image_path = image_prefix_path + image_subpath\n",
    "                    image = cv2.imread(image_path)\n",
    "                    images.append(image)\n",
    "                    angle = center_angle + angle_offsets[image_position]\n",
    "                    angles.append(angle)\n",
    "\n",
    "                    # also add flipped image and angle\n",
    "                    flipped_image = np.fliplr(image)\n",
    "                    flipped_angle_for_image = -angle\n",
    "                    images.append(flipped_image)\n",
    "                    angles.append(flipped_angle_for_image)\n",
    "         \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            #print(X_train.shape)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            #class_weight = sklearn.utils.class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "            sample_weight = sklearn.utils.class_weight.compute_sample_weight('balanced', y_train)\n",
    "            \n",
    "            #print(y_train)\n",
    "            #print(class_weight)\n",
    "            #print(sample_weight)\n",
    "            #print(X_train.shape)\n",
    "            #print(y_train.shape)\n",
    "            #print(class_weight.shape)\n",
    "            #print(sample_weight.shape)\n",
    "\n",
    "            \n",
    "            yield sklearn.utils.shuffle(X_train, y_train, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8323 2081\n"
     ]
    }
   ],
   "source": [
    "#X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2)\n",
    "#print(len(X_train), len(X_validation))\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "print(len(train_samples), len(validation_samples))\n",
    "\n",
    "BATCH_SIZE = 64 # 32 and 64 works mem wise! iteration for 32 is 90s per epoch, around 58 for 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=BATCH_SIZE)\n",
    "validation_generator = generator(validation_samples, batch_size=BATCH_SIZE)\n",
    "\n",
    "# just for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train = np.array(images)\n",
    "#y_train = np.array(measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(X_train.shape)\n",
    "#print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_input_shape=(160,320,3)\n",
    "#batch_size=256\n",
    "#epochs=15\n",
    "\n",
    "# Crop the image - top 50px and bottom 0px\n",
    "# ((top_crop, bottom_crop), (left_crop, right_crop)) = \n",
    "crop_pattern = ((50,25),(0,0))\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x/255.0 - 0.5, input_shape=img_input_shape))\n",
    "model.add(Cropping2D(cropping=crop_pattern))\n",
    "model.add(Conv2D(24,(5,5),padding='valid', activation='relu', strides=(2,2), kernel_regularizer='l2',\n",
    "                kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(padding='same'))\n",
    "model.add(Conv2D(36,(5,5),padding='valid', activation='relu', strides=(2,2), kernel_regularizer='l2',\n",
    "                kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(48,(5,5),padding='valid', activation='relu', strides=(2,2), kernel_regularizer='l2'))\n",
    "model.add(Conv2D(64,(3,3),padding='valid', activation='relu', strides=(1,1), kernel_regularizer='l2'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(400, activation='selu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='selu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(50, activation='selu'))\n",
    "model.add(Dense(10, activation='selu'))\n",
    "model.add(Dense(1,kernel_regularizer='l1', activation='tanh'))\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.layers.core.Lambda'>\n",
      "<class 'keras.layers.convolutional.Cropping2D'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.pooling.MaxPooling2D'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Flatten'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Dropout'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Dropout'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Dense'>\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(type(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 85, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 41, 158, 24)       1824      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 41, 158, 24)       96        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 79, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 38, 36)         21636     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 38, 36)         144       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 17, 48)         43248     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 15, 64)         27712     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 960)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               384400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 524,731\n",
      "Trainable params: 524,611\n",
      "Non-trainable params: 120\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa51cb832e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "model.summary()\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# show model architecture\n",
    "plot_model(model, to_file='model.png')\n",
    "plt.imshow(cv2.imread('model.png'))\n",
    "\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=int(6*len(train_samples)/BATCH_SIZE),\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = int(6*len(validation_samples)/BATCH_SIZE), epochs=4,\n",
    "                    verbose=0, callbacks=[TQDMNotebookCallback(leave_inner=True, leave_outer=True)])\n",
    "\n",
    "# verbose = 0, callbacks=[TQDMNotebookCallback()]\n",
    "                    \n",
    "          #batch_size=batch_size,\n",
    "#          epochs=epochs,\n",
    "#          verbose=1,\n",
    "#          shuffle = True,\n",
    "#          validation_split=0.3)\n",
    "\n",
    "\n",
    "\n",
    "#!pip install h5py\n",
    "import h5py as h5py\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BIG MODEL\n",
    "\n",
    "# nice tutorial for keras:\n",
    "# https://cambridgespark.com/content/tutorials/convolutional-neural-networks-with-keras/index.html\n",
    "\n",
    "img_input_shape=(160,320,3)\n",
    "#batch_size=256\n",
    "#epochs=15\n",
    "\n",
    "# Crop the image - top 50px and bottom 0px\n",
    "# ((top_crop, bottom_crop), (left_crop, right_crop)) = \n",
    "crop_pattern = ((50,25),(0,0))\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x/255.0 - 0.5, input_shape=img_input_shape))\n",
    "model.add(Cropping2D(cropping=crop_pattern, input_shape=img_input_shape))\n",
    "model.add(Conv2D(24,(5,5),padding='valid', activation='relu', strides=(2,2), kernel_regularizer='l2'))\n",
    "model.add(Conv2D(36,(5,5),padding='valid', activation='relu', strides=(2,2), kernel_regularizer='l2'))\n",
    "model.add(Conv2D(48,(5,5),padding='valid', activation='relu', strides=(2,2), kernel_regularizer='l2'))\n",
    "model.add(Conv2D(64,(3,3),padding='valid', activation='relu', strides=(1,1), kernel_regularizer='l2'))\n",
    "model.add(Conv2D(64,(3,3),padding='valid', activation='relu', strides=(1,1), kernel_regularizer='l2'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(400, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1,kernel_regularizer='l1', activation='tanh'))\n",
    "\n",
    "\n",
    "\n",
    "# Adam(lr=0.0001)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# checkpoint\n",
    "#filepath=\"drive_car-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "#print(\"before fit\")\n",
    "#model.fit(X_train, y_train,\n",
    "#          epochs=4,\n",
    "#          batch_size=128, \n",
    "#          shuffle=True,\n",
    "#          validation_data=(X_validation, y_validation), verbose=1)\n",
    "#print(\"after fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {
    "323d6fec38e0448681a5df39411dc1ba": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "443d5b4442ce43ad89ca3dd58df13e24": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "6d79175a7d2242deb6b12c9938fe50da": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "bc03ba5b673c4d21ba40d7055f6e7e41": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "bd16a0f12e804b14aac2e4f4ef03e4b1": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "c343ce70fec04f15be04200afab1c289": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
